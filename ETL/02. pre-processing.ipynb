{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde01dd6",
   "metadata": {},
   "source": [
    "## Importing required libs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b669abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas_ods_reader import read_ods\n",
    "import warnings\n",
    "import re\n",
    "import xlrd\n",
    "from sqlalchemy import create_engine, Table, Column, Integer, String, Float, MetaData\n",
    "from sqlalchemy.exc import IntegrityError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e78a67",
   "metadata": {},
   "source": [
    "### Ignoring errors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc64852",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c809c",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36360dd",
   "metadata": {},
   "source": [
    "# Part I: Intro to files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60e739",
   "metadata": {},
   "source": [
    "### 1.1 - We currently have five distinct data groups: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362b4ec",
   "metadata": {},
   "source": [
    "### 1.1.1 - A. (Sales notifications of all industrialized drugs that have restricted sales in Brazil, like antidepressants, antibiotics, etc.)\n",
    "\n",
    "Example: \"project_data\\raw_anvisa_files\\industrialized_meds\\EDA_Industrializados_201401.csv\"   \n",
    "(93 files in this dir ranging from 2014JAN ~ 2021NOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e86a53",
   "metadata": {},
   "source": [
    "### 1.1.2 - B. (Sales notifications of all compounded drugs that have restricted sales in Brazil, like antidepressants, antibiotics, etc.) \n",
    "\n",
    "Example: \"project_data\\raw_anvisa_files\\manipulated_meds\\EDA_Manipulados_201401.csv\"   \n",
    "(93 files in this dir ranging from 2014JAN ~ 2021NOV)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b7705",
   "metadata": {},
   "source": [
    "### 1.1.3 - C. (Notifications of all obituaries in Brazil)\n",
    "\n",
    "Example: \"project_data\\raw_sim_files\\Mortalidade_Geral_2014.csv\"  \n",
    "(8 files ranging yearly from 2014 to 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a873268",
   "metadata": {},
   "source": [
    "### 1.1.4 - D. (Brazilian cities' total area)\n",
    "Example:\"\\project_data\\ibge_demographic\\br_city_area.ods\"  \n",
    "(A single file from 2023) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100b4d7",
   "metadata": {},
   "source": [
    "### 1.1.5 - E. (Brazilian cities' population)\n",
    "Example: \"\\project_data\\ibge_demographic\\br_city_population.ods\"   \n",
    "(A single file from 2021) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0ed67",
   "metadata": {},
   "source": [
    "# Part II: Checking column names in all paths:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded15d6",
   "metadata": {},
   "source": [
    "### 2.1 Defining all \"folder\" paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2151aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\raw_anvisa_files\\industrialized_meds\",\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\raw_anvisa_files\\manipulated_meds\",\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\raw_sim_files\",\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\ibge_demographic\\area\",\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\ibge_demographic\\population\"   \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28669a35",
   "metadata": {},
   "source": [
    "### 2.2 Defining a function that iterates through files inside \"folder_paths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff8a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(folder_path, extension=None):\n",
    "    if extension:\n",
    "        return [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(extension)]\n",
    "    else:\n",
    "        return [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de1feb",
   "metadata": {},
   "source": [
    "### 2.3 Defining a function to verify the integrity of column names and return the file indices of columns that have different names.\n",
    "\n",
    "#### I have also included another feature, namely chunk_size, with the goal of optimizing resource usage by not opening entire files at once. (Using 1k chunks is more than enough to retrieve column names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56517c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns_similarity(folder_paths, chunk_size=1000):\n",
    "    for folder_path in folder_paths:\n",
    "        file_paths = get_file_paths(folder_path)\n",
    "\n",
    "        if not file_paths:\n",
    "            raise ValueError(f\"Folder {folder_path} is empty. No files to process.\")\n",
    "\n",
    "        # Extract only the folder name\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        # Dictionary to store column names for each file within the folder\n",
    "        folder_columns = {}\n",
    "\n",
    " # Process each file within the folder\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith(\".csv\"):\n",
    "            reader = pd.read_csv(file_path, delimiter=';', encoding='latin-1', quotechar='\"', chunksize=chunk_size)\n",
    "            df_chunk = next(reader)\n",
    "            folder_columns[file_path] = df_chunk.columns.tolist()\n",
    "        elif file_path.endswith(\".xls\"):\n",
    "            for df_chunk in read_excel_in_chunks(file_path, chunk_size=chunk_size):\n",
    "                folder_columns[file_path] = df_chunk.columns.tolist()\n",
    "                break  # Only process the first chunk for column names\n",
    "        elif file_path.endswith(\".xlsx\"):\n",
    "            # For .xlsx files, continue using openpyxl\n",
    "            reader = pd.read_excel(file_path, engine='openpyxl', chunksize=chunk_size)\n",
    "            df_chunk = next(reader)\n",
    "            folder_columns[file_path] = df_chunk.columns.tolist()\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {file_path}. Skipping.\")\n",
    "\n",
    "    # Check if all files within the folder have the same columns\n",
    "    if len(set(map(tuple, folder_columns.values()))) == 1:\n",
    "        print(f\"\\U0001F7E2 All files in folder \\\"{folder_name}\\\" have the same columns.\\n\")  # Green circle emoji\n",
    "        return {'files_with_same_columns': list(range(len(file_paths))), 'files_with_different_columns': []}\n",
    "    else:\n",
    "        print(f\"\\U0001F534 Files in folder \\\"{folder_name}\\\" have different columns.\\n\")  # Red circle emoji\n",
    "\n",
    "        # Group files based on columns\n",
    "        grouped_files = {}\n",
    "        for index, (path, columns) in enumerate(folder_columns.items()):\n",
    "            key = tuple(columns)\n",
    "            if key not in grouped_files:\n",
    "                grouped_files[key] = [index]\n",
    "            else:\n",
    "                grouped_files[key].append(index)\n",
    "\n",
    "        return {'files_with_same_columns': grouped_files.get(next(iter(grouped_files)), []),\n",
    "                'files_with_different_columns': [index for index in range(len(file_paths)) if index not in grouped_files.get(next(iter(grouped_files)), [])]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f65f38",
   "metadata": {},
   "source": [
    "### 2.4 Usage example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17e198",
   "metadata": {},
   "source": [
    "### - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f03feffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the index of the folder to be processed (e.g., folder_paths[0] for the first folder)\n",
    "selected_folder_path_a = folder_paths[0]\n",
    "\n",
    "# Get all files in the selected folder\n",
    "all_files_a = get_file_paths(selected_folder_path_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3c9cf3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ All files in folder \"industrialized_meds\" have the same columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93], 'files_with_different_columns': []}\n"
     ]
    }
   ],
   "source": [
    "# Print the columns of the files in the selected folder\n",
    "result_columns = check_columns_similarity([selected_folder_path_a])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe4dfd",
   "metadata": {},
   "source": [
    "### - B)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1451191",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folder_path_b = folder_paths[1]\n",
    "all_files_b = get_file_paths(selected_folder_path_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24dd789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ All files in folder \"manipulated_meds\" have the same columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93], 'files_with_different_columns': []}\n"
     ]
    }
   ],
   "source": [
    "result_columns = check_columns_similarity([selected_folder_path_b])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f452d",
   "metadata": {},
   "source": [
    "### - C)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41ca4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folder_path_c = folder_paths[2]\n",
    "all_files_c = get_file_paths(selected_folder_path_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f56adf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ Files in folder \"raw_sim_files\" have different columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0, 1, 2, 3], 'files_with_different_columns': [4, 5, 6, 7]}\n"
     ]
    }
   ],
   "source": [
    "result_columns = check_columns_similarity([selected_folder_path_c])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93e65e",
   "metadata": {},
   "source": [
    "## The files in the 'C' folder don't have standardized columns.\n",
    "#### We can keep this in mind when looking for errors during the subsequent data insertion into MSSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a2794",
   "metadata": {},
   "source": [
    "### - D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6e88c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folder_path_d = folder_paths[3]\n",
    "all_files_d = get_file_paths(selected_folder_path_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ca3ee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ All files in folder \"area\" have the same columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0], 'files_with_different_columns': []}\n"
     ]
    }
   ],
   "source": [
    "result_columns = check_columns_similarity([selected_folder_path_d])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8820a48",
   "metadata": {},
   "source": [
    "### - E)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folder_path_e = folder_paths[4]\n",
    "all_files_e = get_file_paths(selected_folder_path_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46bd84bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ All files in folder \"population\" have the same columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0], 'files_with_different_columns': []}\n"
     ]
    }
   ],
   "source": [
    "result_columns = check_columns_similarity([selected_folder_path_e])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e03b59",
   "metadata": {},
   "source": [
    "# Part III: Joining D and E files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef349b",
   "metadata": {},
   "source": [
    "### 3.1 - Doing a inner join with D and E\n",
    "\n",
    "D represents the local directory containing data for the total area of cities, while E represents the file containing the total population of cities. Since both files are relatively small, it makes sense to perform this operation with Pandas, conducting an inner join based on the city code, and subsequently modifying it for compatibility with an SQL-supported extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "986586e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ID CD_UF     NM_UF NM_UF_SIGLA   CD_MUN                 NM_MUN  AR_MUN_2022  \\\n",
      "0  1    11  Rond√¥nia          RO  1100015  Alta Floresta D'Oeste     7067.127   \n",
      "1  2    11  Rond√¥nia          RO  1100023              Ariquemes     4426.571   \n",
      "2  3    11  Rond√¥nia          RO  1100031                 Cabixi     1314.352   \n",
      "3  4    11  Rond√¥nia          RO  1100049                 Cacoal     3793.000   \n",
      "4  5    11  Rond√¥nia          RO  1100056             Cerejeiras     2783.300   \n",
      "\n",
      "   POPULA√á√ÉO ESTIMADA   \n",
      "0              22516.0  \n",
      "1             111148.0  \n",
      "2               5067.0  \n",
      "3              86416.0  \n",
      "4              16088.0  \n"
     ]
    }
   ],
   "source": [
    "# 3.1.1 Reading the data\n",
    "df_d = read_ods(folder_path_d, 1)\n",
    "df_e = read_ods(folder_path_e, 1)\n",
    "\n",
    "# 3.1.2 Merge the DataFrames based on the common column 'NM_MUN' in df_d and 'NOME DO MUNIC√çPIO' in df_e\n",
    "merged_df = df_d.merge(df_e[['NOME DO MUNIC√çPIO', ' POPULA√á√ÉO ESTIMADA ']], left_on='NM_MUN', right_on='NOME DO MUNIC√çPIO', how='inner')\n",
    "\n",
    "# 3.1.3 Drop the redundant 'NOME DO MUNIC√çPIO' column\n",
    "merged_df.drop(columns=['NOME DO MUNIC√çPIO'], inplace=True)\n",
    "\n",
    "# 3.1.4 Rename the merged DataFrame to 'city_area_population'\n",
    "city_area_population = merged_df\n",
    "\n",
    "# 3.1.5: Save the DataFrame as a CSV file\n",
    "\n",
    "# 3.1.6 Save the 'city_area_population' DataFrame as a CSV file in the same directory\n",
    "\n",
    "output_file_path= r\"(...)\\My_2ndEDA_Drug_Resistent_Bacteria\\project_data\\ibge_demographic\\city_area_population.csv\"\n",
    "\n",
    "city_area_population.to_csv(output_file_path, index=False)\n",
    "\n",
    "# 3.1.7 Display the first few rows of the DataFrame\n",
    "\n",
    "# 3.1.8 Print the first few rows of the 'city_area_population' DataFrame\n",
    "print(city_area_population.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd9e21",
   "metadata": {},
   "source": [
    "# Part IV: Getting column dtypes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270c089",
   "metadata": {},
   "source": [
    "### 4.1.1 - For \"a\" files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eceb5a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANO_VENDA                   int64\n",
      "MES_VENDA                   int64\n",
      "UF_VENDA                   object\n",
      "MUNICIPIO_VENDA            object\n",
      "PRINCIPIO_ATIVO            object\n",
      "DESCRICAO_APRESENTACAO     object\n",
      "QTD_VENDIDA                 int64\n",
      "UNIDADE_MEDIDA             object\n",
      "CONSELHO_PRESCRITOR        object\n",
      "UF_CONSELHO_PRESCRITOR     object\n",
      "TIPO_RECEITUARIO          float64\n",
      "CID10                      object\n",
      "SEXO                      float64\n",
      "IDADE                     float64\n",
      "UNIDADE_IDADE             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "if all_files_a:\n",
    "    # Read the first CSV file in the list into a DataFrame\n",
    "    df = pd.read_csv(all_files_a[0], delimiter=';', encoding='latin-1', quotechar='\"')\n",
    "\n",
    "    # Get the data types of the columns\n",
    "    column_types = df.dtypes\n",
    "\n",
    "    # Print the data types\n",
    "    print(column_types)\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf273b",
   "metadata": {},
   "source": [
    "### 4.1.2 - For \"b\" files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab456676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANO_VENDA                           int64\n",
      "MES_VENDA                           int64\n",
      "UF_VENDA                           object\n",
      "MUNICIPIO_VENDA                    object\n",
      "PRINCIPIO_ATIVO                    object\n",
      "QTD_ATIVO_POR_UNID_FARMACOTEC      object\n",
      "UNIDADE_MEDIDA_PRINCIPIO_ATIVO     object\n",
      "QTD_UNIDADE_FARMACOTECNICA         object\n",
      "TIPO_UNIDADE_FARMACOTECNICA        object\n",
      "CID10                              object\n",
      "SEXO                              float64\n",
      "IDADE                             float64\n",
      "UNIDADE_IDADE                     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "if all_files_b:\n",
    "    # Read the second CSV file in the list into a DataFrame\n",
    "    df = pd.read_csv(all_files_b[1], delimiter=';', encoding='latin-1', quotechar='\"')\n",
    "\n",
    "    # Get the data types of the columns\n",
    "    column_types = df.dtypes\n",
    "\n",
    "    # Print the data types\n",
    "    print(column_types)\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f90557",
   "metadata": {},
   "source": [
    "### 4.1.3 - For \"c\" files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b1eb0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTADOR        int64\n",
      "ORIGEM          int64\n",
      "TIPOBITO        int64\n",
      "DTOBITO         int64\n",
      "HORAOBITO     float64\n",
      "NATURAL       float64\n",
      "CODMUNNATU    float64\n",
      "DTNASC        float64\n",
      "IDADE           int64\n",
      "SEXO            int64\n",
      "RACACOR       float64\n",
      "ESTCIV        float64\n",
      "ESC           float64\n",
      "ESC2010       float64\n",
      "SERIESCFAL    float64\n",
      "OCUP          float64\n",
      "CODMUNRES       int64\n",
      "LOCOCOR         int64\n",
      "CODESTAB      float64\n",
      "ESTABDESCR    float64\n",
      "CODMUNOCOR      int64\n",
      "IDADEMAE      float64\n",
      "ESCMAE        float64\n",
      "ESCMAE2010    float64\n",
      "SERIESCMAE    float64\n",
      "OCUPMAE       float64\n",
      "QTDFILVIVO    float64\n",
      "QTDFILMORT    float64\n",
      "GRAVIDEZ      float64\n",
      "SEMAGESTAC    float64\n",
      "GESTACAO      float64\n",
      "PARTO         float64\n",
      "OBITOPARTO    float64\n",
      "PESO          float64\n",
      "TPMORTEOCO    float64\n",
      "OBITOGRAV     float64\n",
      "OBITOPUERP    float64\n",
      "ASSISTMED     float64\n",
      "EXAME         float64\n",
      "CIRURGIA      float64\n",
      "NECROPSIA     float64\n",
      "LINHAA         object\n",
      "LINHAB         object\n",
      "LINHAC         object\n",
      "LINHAD         object\n",
      "LINHAII        object\n",
      "CAUSABAS       object\n",
      "CB_PRE        float64\n",
      "COMUNSVOIM    float64\n",
      "DTATESTADO    float64\n",
      "CIRCOBITO     float64\n",
      "ACIDTRAB      float64\n",
      "FONTE         float64\n",
      "NUMEROLOTE    float64\n",
      "TPPOS          object\n",
      "DTINVESTIG    float64\n",
      "CAUSABAS_O     object\n",
      "DTCADASTRO    float64\n",
      "ATESTANTE     float64\n",
      "STCODIFICA     object\n",
      "CODIFICADO     object\n",
      "VERSAOSIST     object\n",
      "VERSAOSCB     float64\n",
      "FONTEINV      float64\n",
      "DTRECEBIM     float64\n",
      "ATESTADO       object\n",
      "DTRECORIGA      int64\n",
      "CAUSAMAT       object\n",
      "ESCMAEAGR1    float64\n",
      "ESCFALAGR1    float64\n",
      "STDOEPIDEM    float64\n",
      "STDONOVA        int64\n",
      "DIFDATA         int64\n",
      "NUDIASOBCO    float64\n",
      "NUDIASOBIN    float64\n",
      "DTCADINV      float64\n",
      "TPOBITOCOR    float64\n",
      "DTCONINV      float64\n",
      "FONTES         object\n",
      "TPRESGINFO    float64\n",
      "TPNIVELINV     object\n",
      "NUDIASINF     float64\n",
      "DTCADINF      float64\n",
      "MORTEPARTO    float64\n",
      "DTCONCASO     float64\n",
      "FONTESINF     float64\n",
      "ALTCAUSA      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Setting pandas option that will let us see all columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "if all_files_c:\n",
    "    # Read the third CSV file in the list into a DataFrame\n",
    "    df = pd.read_csv(all_files_c[2], delimiter=';', encoding='latin-1', quotechar='\"')\n",
    "\n",
    "    # Get the data types of the columns\n",
    "    column_types = df.dtypes\n",
    "\n",
    "    # Print the data types\n",
    "    print(column_types)\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb39e9d",
   "metadata": {},
   "source": [
    "### 4.1.4 - For \"F\" files:\n",
    "#### (We refer to the output obtained by combining \"E\" and \"D\" as \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "038e96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the \"file_path_f\": \n",
    "\n",
    "file_path_f = r\"(...)\\Drug_Resistant_Bacteria\\project_data\\ibge_demographic\\city_area_population.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ffbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,CD_UF,NM_UF,NM_UF_SIGLA,CD_MUN,NM_MUN,AR_MUN_2022, POPULA√á√ÉO ESTIMADA     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# L√™ o arquivo CSV em um DataFrame\n",
    "df = pd.read_csv(file_path_f, delimiter=';', encoding='utf-8', quotechar='\"')\n",
    "\n",
    "# Obt√©m os tipos de dados das colunas\n",
    "column_types = df.dtypes\n",
    "\n",
    "# Imprime os tipos de dados\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a90491",
   "metadata": {},
   "source": [
    "# Part V: \"Dropping unnecessary columns\"\n",
    "#### (Or selecting desired columns, you choose, lol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b367c",
   "metadata": {},
   "source": [
    "### 5.1 - Defining desired columns for each file types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf84f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns_a = ['ANO_VENDA', 'MES_VENDA', 'UF_VENDA', 'MUNICIPIO_VENDA', 'PRINCIPIO_ATIVO', 'QTD_VENDIDA', 'UNIDADE_MEDIDA',\n",
    " 'CID10', 'SEXO', 'IDADE', 'UNIDADE_IDADE']\n",
    "\n",
    "desired_columns_b = ['ANO_VENDA', 'MES_VENDA', 'UF_VENDA', 'MUNICIPIO_VENDA', 'PRINCIPIO_ATIVO', \n",
    "                      'CID10', 'SEXO', 'IDADE', 'UNIDADE_IDADE']\n",
    "\n",
    "desired_columns_c = ['CONTADOR', 'DTOBITO', 'NATURAL', 'CODMUNNATU', 'IDADE', 'SEXO', 'CODMUNRES', 'CODESTAB',\n",
    " 'CODMUNOCOR', 'LINHAA', 'LINHAB', 'LINHAC', 'LINHAD', 'LINHAII', 'CAUSABAS','CAUSABAS_O',\n",
    " 'DTCADASTRO', 'ATESTADO', 'CAUSAMAT','STDOEPIDEM']\n",
    "\n",
    "desired_columns_d = ['ID','CD_UF','NM_UF','NM_UF_SIGLA','CD_MUN','NM_MUN','AR_MUN_2022']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ac978",
   "metadata": {},
   "source": [
    "# Part VI: Connecting to SQL server and creating target tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1044d",
   "metadata": {},
   "source": [
    "### 6.1 - Defining function that creates table inside mssql: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(engine, table_name, column_definitions, if_exists='fail'):\n",
    "    metadata = MetaData()\n",
    "    table = Table(table_name, metadata, *[Column(name, col_type) for name, col_type in column_definitions.items()])\n",
    "\n",
    "    try:\n",
    "        metadata.create_all(engine)\n",
    "        print(f'Table \"{table_name}\" created successfully.')\n",
    "    except IntegrityError as e:\n",
    "        if if_exists == 'replace':\n",
    "            metadata.drop_all(engine)\n",
    "            metadata.create_all(engine)\n",
    "            print(f'Table \"{table_name}\" replaced successfully.')\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d3ec9",
   "metadata": {},
   "source": [
    "### 6.2 - Defining engine parameters to connect to sql instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_server', 'your_database', 'your_user', and 'your_password' with the correct information\n",
    "engine = create_engine(r'mssql+pyodbc://\"your_server\"/\"your_database\"?driver=ODBC+Driver+17+for+SQL+\"your_password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2306c",
   "metadata": {},
   "source": [
    "### 6.3 - Defining which dtypes for creating each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "288724b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of table names\n",
    "table_names = ['industrialized_meds', 'manipulated_meds', 'death_obituaries', 'demographics']  # Add desired column names here\n",
    "\n",
    "industrialized_meds = {\n",
    "    'ANO_VENDA': Integer(),\n",
    "    'MES_VENDA': Integer(),\n",
    "    'UF_VENDA': String(),\n",
    "    'MUNICIPIO_VENDA': String(),\n",
    "    'PRINCIPIO_ATIVO': String(),\n",
    "    'DESCRICAO_APRESENTACAO': String(),\n",
    "    'QTD_VENDIDA': Integer(),\n",
    "    'UNIDADE_MEDIDA': String(),\n",
    "    'CONSELHO_PRESCRITOR': String(),\n",
    "    'UF_CONSELHO_PRESCRITOR': String(),\n",
    "    'TIPO_RECEITUARIO': Float(),\n",
    "    'CID10': String(),\n",
    "    'SEXO': Float(),\n",
    "    'IDADE': Float(),\n",
    "    'UNIDADE_IDADE': Float()\n",
    "}\n",
    "\n",
    "manipulated_meds = {\n",
    "    'ANO_VENDA': Integer(),\n",
    "    'MES_VENDA': Integer(),\n",
    "    'UF_VENDA': String(),\n",
    "    'MUNICIPIO_VENDA': String(),\n",
    "    'PRINCIPIO_ATIVO': String(),\n",
    "    'QTD_ATIVO_POR_UNID_FARMACOTEC': String(),\n",
    "    'UNIDADE_MEDIDA_PRINCIPIO_ATIVO': String(),\n",
    "    'QTD_UNIDADE_FARMACOTECNICA': String(),\n",
    "    'TIPO_UNIDADE_FARMACOTECNICA': String(),\n",
    "    'CID10': String(),\n",
    "    'SEXO': Float(),\n",
    "    'IDADE': Float(),\n",
    "    'UNIDADE_IDADE': Float()\n",
    "}\n",
    "\n",
    "death_obituaries = {\n",
    " \n",
    "    'DTOBITO': Integer(),\n",
    "    'NATURAL': Integer(),\n",
    "    'CODMUNNATU': Integer(),\n",
    "    'IDADE': Integer(),\n",
    "    'SEXO': Integer(),\n",
    "    'CODMUNRES':Integer(),\n",
    "    'LOCOCOR': Integer(),  \n",
    "    'CODMUNOCOR':Integer(),\n",
    "    'LINHAA': String(),\n",
    "    'LINHAB': String(),\n",
    "    'LINHAC': String(),\n",
    "    'LINHAD': String(),\n",
    "    'LINHAII': String(),\n",
    "    'CAUSABAS': String(),       \n",
    "    'ATESTADO': String(),\n",
    "    'STDOEPIDEM': Integer()\n",
    "}\n",
    "\n",
    "demographics = {\n",
    "    'ID': Integer(),\n",
    "    'CD_UF': Integer(),\n",
    "    'NM_UF': String(),\n",
    "    'NM_UF_SIGLA': String(),\n",
    "    'CD_MUN': Integer(),\n",
    "    'NM_MUN': String(),\n",
    "    'AR_MUN_2022': Float(),\n",
    "    'POPULA√á√ÉO ESTIMADA': String()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8b3e9",
   "metadata": {},
   "source": [
    "### 5.3 - Calling function to create tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c670d013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table \"industrialized_meds\" created successfully.\n",
      "Table \"manipulated_meds\" created successfully.\n",
      "Table \"death_obituaries\" created successfully.\n",
      "Table \"demographics\" created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over table names and create tables\n",
    "for table_name, column_definitions in zip(table_names, [industrialized_meds, manipulated_meds, death_obituaries, demographics]):\n",
    "    create_table(engine, table_name, column_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b6415",
   "metadata": {},
   "source": [
    "# Part VI: Counting number of rows in each file to verify data integrity after insertions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3e408",
   "metadata": {},
   "source": [
    "### 6.1 - Defining the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84ed68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_in_file(file_path, encoding='latin-1'):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter=';', encoding=encoding, quoting=csv.QUOTE_ALL)\n",
    "        row_count = len(df)\n",
    "        return row_count\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError: Could not read the file {file_path}.\")\n",
    "        return 0\n",
    "\n",
    "def create_report(folder_path, output_file_path, encoding='latin-1'):\n",
    "    row_counts = {}\n",
    "\n",
    "    # Get all CSV files in the folder\n",
    "    csv_files = get_file_paths(folder_path, extension='.csv')\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        row_count = count_rows_in_file(file_path, encoding)\n",
    "\n",
    "        # Extract year and month from the filename\n",
    "        match = re.search(r'_(\\d{4})(\\d{2})?\\.csv', os.path.basename(file_path))  # Updated regex pattern\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            month = match.group(2) or ''\n",
    "            key = f\"{year};{month}\"\n",
    "\n",
    "            row_counts[key] = row_count\n",
    "\n",
    "    # Convert row_counts to a DataFrame\n",
    "    report_df = pd.DataFrame(list(row_counts.items()), columns=[\"year;month\", \"count\"])\n",
    "\n",
    "    # Split the \"year;month\" column into two columns: \"year\" and \"month\"\n",
    "    report_df[[\"year\", \"month\"]] = report_df[\"year;month\"].str.split(\";\", expand=True)\n",
    "\n",
    "    # Now you can rename the columns\n",
    "    report_df = report_df[[\"year\", \"month\", \"count\"]]\n",
    "\n",
    "    # Save the report to the CSV file\n",
    "    report_df.to_csv(output_file_path, index=False, sep=';', decimal=',', header=True)\n",
    "\n",
    "    # Display the first few rows of the report\n",
    "    print(report_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ecf7a",
   "metadata": {},
   "source": [
    "## 6.2 Calling the report function:\n",
    "### We need to provide two arguments to generate the report that counts number of rows per file in each different dir: \n",
    "\n",
    "\"folder_path\" and \"output_file_path\": Example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025024b",
   "metadata": {},
   "source": [
    "### 'A' folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01abd1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month    count\n",
      "0  2014    01  4663124\n",
      "1  2014    02  4461102\n",
      "2  2014    03  4770799\n",
      "3  2014    04  4959036\n",
      "4  2014    05  5257119\n"
     ]
    }
   ],
   "source": [
    "output_report_path_a = r\"(...)\\Drug_Resistant_Bacteria\\project_data\\validation_reports\\validation_report_a.csv\"\n",
    "create_report(selected_folder_path_a, output_report_path_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bf5c7",
   "metadata": {},
   "source": [
    "### 'B' folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7cc71ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month   count\n",
      "0  2014    01  272874\n",
      "1  2014    02  277087\n",
      "2  2014    03  272527\n",
      "3  2014    04  283229\n",
      "4  2014    05  295848\n"
     ]
    }
   ],
   "source": [
    "output_report_path_a = r\"(...)\\Drug_Resistant_Bacteria\\project_data\\validation_reports\\validation_report_b.csv\"\n",
    "create_report(selected_folder_path_b, output_report_path_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629afbcb",
   "metadata": {},
   "source": [
    "### 'C' folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48914888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month    count\n",
      "0  2014        1227040\n",
      "1  2015        1264176\n",
      "2  2016        1309775\n",
      "3  2017        1312664\n",
      "4  2018        1316720\n"
     ]
    }
   ],
   "source": [
    "output_report_path_c = r\"(...)\\Drug_Resistant_Bacteria\\project_data\\validation_reports\\validation_report_c.csv\"\n",
    "create_report(selected_folder_path_c, output_report_path_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b32bd2",
   "metadata": {},
   "source": [
    "### 'F' folder: \n",
    "\"The file in 'f' dir does not contain either years or months, so we only need to count the number of rows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d0caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in the file: 6225\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(file_path_f)\n",
    "    line_count = len(df)\n",
    "    print(f\"Total lines in the file: {line_count}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The file is empty or does not contain data.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error parsing the CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a356e",
   "metadata": {},
   "source": [
    "# Part VII: Inserting files into respectives tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c02bc",
   "metadata": {},
   "source": [
    "### 7.1 Defining function that inserts files into mssql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c74b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_csv_files_into_mssql(folder_path, table_name, desired_columns, engine, chunk_size=5000):\n",
    "    column_index_mapping = {col_name: i for i, col_name in enumerate(desired_columns)}\n",
    "\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    file_paths = get_file_paths(folder_path, extension=\".csv\")\n",
    "\n",
    "    # Abrir a conex√£o uma vez para todas as inser√ß√µes\n",
    "    with engine.connect() as conn:\n",
    "        for file_path in file_paths:\n",
    "            file_name = os.path.basename(file_path)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            chunks = pd.read_csv(file_path, delimiter=';', encoding='latin-1', quotechar='\"', chunksize=chunk_size)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                df_selected_columns = chunk[[col for col in chunk.columns if col in column_index_mapping]]\n",
    "\n",
    "                # Usar a conex√£o para inser√ß√£o\n",
    "                df_selected_columns.to_sql(table_name, con=conn, if_exists='append', index=False, method='multi')\n",
    "\n",
    "                print(f'Chunk {i+1} inserted for {file_name}.')\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            print(f'Time elapsed for {file_name}: {elapsed_time} seconds')\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    elapsed_time_total = end_time_total - start_time_total\n",
    "\n",
    "    print(f'Total time elapsed: {elapsed_time_total} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b152c",
   "metadata": {},
   "source": [
    "### 7.2 - Inserting files into 'industrialized_meds' table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2505459",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_a = insert_csv_files_into_mssql_(selected_folder_path_a, 'industrialized_meds', desired_columns_a.keys(), engine, chunk_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c88e9c",
   "metadata": {},
   "source": [
    "### 7.3 - Inserting files into 'manipulated_meds' table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6907ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_b = insert_csv_files_into_mssql(selected_folder_path_b, 'manipulated_meds', desired_columns_b.keys(), engine, chunk_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a34b1",
   "metadata": {},
   "source": [
    "### 7.4 - Inserting Files into the 'death_obituaries' table:\n",
    "\n",
    "#### (Initially, my plan was to insert all files directly into the death_obituaries table in the SQL Server and perform data cleaning within the SQL instance. However, this approach was proving to be very time-consuming, and multiple attempts to insert into MSSQL were failing. Therefore, I decided to adopt a different approach, which included the following steps:\n",
    "\n",
    "#### 1- Reducing Sample Size: I used Pandas to work with a reduced sample of the data, aiming to optimize processing and analysis,\n",
    "#### taking only absolutely neccessary columns\n",
    "#### 2 - Data preparation: Converted specific numeric columns to integers and rounded the values to ensure data consistency.\n",
    "#### 3 - Standardized the dates to a uniform format, facilitating database insertion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting to display more columns\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "# Necessary columns\n",
    "necessary_columns = ['DTOBITO', 'NATURAL', 'CODMUNNATU', 'IDADE', 'SEXO',\n",
    "                     'CODMUNRES', 'LOCOCOR', 'CODMUNOCOR', 'LINHAA', 'LINHAB',\n",
    "                     'LINHAC', 'LINHAD', 'LINHAII', 'CAUSABAS', 'ATESTADO', 'STDOEPIDEM']\n",
    "\n",
    "def convert_to_integer(df, columns):\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].fillna(0).round().astype(int)\n",
    "    return df\n",
    "\n",
    "def standardize_date(column):\n",
    "    column = column.astype(str)\n",
    "    column = column.str.pad(width=8, side='left', fillchar='0')\n",
    "    column = pd.to_datetime(column, format='%d%m%Y', errors='coerce')\n",
    "    return column\n",
    "\n",
    "def process_and_insert_csv_folder_c(file_path, table_name, engine, chunk_size=5000):\n",
    "    csv_reader = pd.read_csv(file_path, delimiter=';', encoding='latin-1', quotechar='\"', chunksize=chunk_size)\n",
    "    for chunk in csv_reader:\n",
    "        # Process the chunk\n",
    "        chunk_filtered = chunk[necessary_columns]\n",
    "        chunk_filtered = convert_to_integer(chunk_filtered, ['NATURAL', 'CODMUNNATU', 'CODMUNRES', 'LOCOCOR', 'CODMUNOCOR', 'IDADE', 'SEXO', 'STDOEPIDEM'])\n",
    "        chunk_filtered['DTOBITO'] = standardize_date(chunk_filtered['DTOBITO'])\n",
    "\n",
    "        # Insert processed data into the table\n",
    "        chunk_filtered.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Selected folder and file list\n",
    "# selected_folder_path_c = folder_paths[2]\n",
    "# all_files_c = get_file_paths(selected_folder_path_c)\n",
    "\n",
    "# Process and insert each CSV file from the selected folder\n",
    "for file_path in all_files_c:\n",
    "    process_and_insert_csv_folder_c(file_path, 'death_obituaries', engine, chunk_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a1bce",
   "metadata": {},
   "source": [
    "### 7.5 - Inserting F file into 'demographics' table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429dd9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_f = insert_csv_files_into_mssql(folder_path_f, 'demographics', desired_columns_f, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5bdaf",
   "metadata": {},
   "source": [
    "# Part VIII: Validating all the insertions on the sql instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43ee90",
   "metadata": {},
   "source": [
    "### 8.1 - Defining functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20f4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.1.1 - Function to execute SQL query and return a DataFrame\n",
    "\n",
    "def execute_sql_query(sql_query, engine):\n",
    "    return pd.read_sql(sql_query, engine)\n",
    "\n",
    "#8.1.2 - Function to read a CSV and adjust the 'count' column\n",
    "\n",
    "def read_and_adjust_csv(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "    df['count'] = df['count'] - 1\n",
    "    df.rename(columns={'count': 'count_csv'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "#8.1.3 - Function to merge and check if the counts match\n",
    "\n",
    "def merge_and_compare(df_sql, df_csv, join_columns):\n",
    "    merged_df = pd.merge(df_sql, df_csv, on=join_columns, how='outer')\n",
    "    merged_df['count_match'] = merged_df['count_sql'] == merged_df['count_csv']\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1207979",
   "metadata": {},
   "source": [
    "### 8.2 - Validating 'industrialized_meds': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4488136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    count_sql  year  month  count_csv  count_match\n",
      "0     4663123  2014      1    4663123         True\n",
      "1     4461101  2014      2    4461101         True\n",
      "2     4770798  2014      3    4770798         True\n",
      "3     4959035  2014      4    4959035         True\n",
      "4     5257118  2014      5    5257118         True\n",
      "..        ...   ...    ...        ...          ...\n",
      "89    6300744  2021      7    6300744         True\n",
      "90    6263825  2021      8    6263825         True\n",
      "91    6131691  2021      9    6131691         True\n",
      "92    5989708  2021     10    5989708         True\n",
      "93    2785568  2021     11    2785568         True\n",
      "\n",
      "[94 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "\n",
    "sql_query_a = \"\"\"\n",
    "SELECT COUNT(*) AS count_sql, ANO_VENDA AS year, MES_VENDA AS month\n",
    "FROM dbo.industrialized_meds\n",
    "GROUP BY ANO_VENDA, MES_VENDA\n",
    "ORDER BY ANO_VENDA, MES_VENDA;\n",
    "\"\"\"\n",
    "validation_report_sql_a = execute_sql_query(sql_query_a, engine)\n",
    "\n",
    "validation_report_csv_a = read_and_adjust_csv(output_report_path_a)\n",
    "\n",
    "merged_df_a = merge_and_compare(validation_report_sql_a, validation_report_csv_a, ['year', 'month'])\n",
    "\n",
    "print(merged_df_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992cc205",
   "metadata": {},
   "source": [
    "### 8.3 - Validating 'manipulated_meds': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db85be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    count_sql  year  month  count_csv  count_match\n",
      "0      272873  2014      1     272873         True\n",
      "1      277086  2014      2     277086         True\n",
      "2      272526  2014      3     272526         True\n",
      "3      283228  2014      4     283228         True\n",
      "4      295847  2014      5     295847         True\n",
      "..        ...   ...    ...        ...          ...\n",
      "89     391425  2021      7     391425         True\n",
      "90     394156  2021      8     394156         True\n",
      "91     379754  2021      9     379754         True\n",
      "92     350748  2021     10     350748         True\n",
      "93     246909  2021     11     246909         True\n",
      "\n",
      "[94 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# SQL Query \n",
    "\n",
    "sql_query_b = \"\"\"\n",
    "SELECT COUNT(*) AS count_sql, ANO_VENDA AS year, MES_VENDA AS month\n",
    "FROM dbo.manipulated_med\n",
    "GROUP BY ANO_VENDA, MES_VENDA\n",
    "ORDER BY ANO_VENDA, MES_VENDA;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "validation_report_sql_b = execute_sql_query(sql_query_b, engine)\n",
    "\n",
    "validation_report_csv_b = read_and_adjust_csv(output_report_path_b)\n",
    "\n",
    "merged_df_b = merge_and_compare(validation_report_sql_b, validation_report_csv_b, ['year', 'month'])\n",
    "\n",
    "print(merged_df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d15343",
   "metadata": {},
   "source": [
    "### 8.4 - Validating 'death_obituaries': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfcabca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  count_sql  month  count_csv  count_match\n",
      "0  2014    1227039    NaN    1227039         True\n",
      "1  2015    1264175    NaN    1264175         True\n",
      "2  2016    1309774    NaN    1309774         True\n",
      "3  2017    1312663    NaN    1312663         True\n",
      "4  2018    1316719    NaN    1316719         True\n",
      "5  2019    1349801    NaN    1349801         True\n",
      "6  2020    1556824    NaN    1556824         True\n",
      "7  2021    1832649    NaN    1832649         True\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "\n",
    "sql_query_c = \"\"\"\n",
    "SELECT YEAR(DTOBITO) AS year, COUNT(*) AS count_sql\n",
    "FROM dbo.death_obituaries\n",
    "GROUP BY YEAR(DTOBITO)\n",
    "ORDER BY year;\n",
    "\"\"\"\n",
    "\n",
    "validation_report_sql_c = execute_sql_query(sql_query_c, engine)\n",
    "\n",
    "validation_report_csv_c = read_and_adjust_csv(output_report_path_c)\n",
    "\n",
    "merged_df_c = merge_and_compare(validation_report_sql_c, validation_report_csv_c, ['year'])\n",
    "\n",
    "print(merged_df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a246d",
   "metadata": {},
   "source": [
    "### 8.5 - Validating 'demographics': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad88dfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas: 6225\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "sql_query_f = \"\"\"\n",
    "SELECT COUNT(*) AS total_count\n",
    "FROM dbo.demographics\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and store the result in a DataFrame\n",
    "validation_report_sql_f = execute_sql_query(sql_query_f, engine)\n",
    "\n",
    "# Print the total number of rows counted\n",
    "total_count = validation_report_sql_f.iloc[0]['total_count']\n",
    "print(f\"Total rows: {total_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
