{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde01dd6",
   "metadata": {},
   "source": [
    "## Importing required libs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b669abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas_ods_reader import read_ods\n",
    "import warnings\n",
    "import re\n",
    "import xlrd\n",
    "from sqlalchemy import create_engine, Table, Column, Integer, String, Float, MetaData\n",
    "from sqlalchemy.exc import IntegrityError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e78a67",
   "metadata": {},
   "source": [
    "### Ignoring errors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cc64852",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c809c",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36360dd",
   "metadata": {},
   "source": [
    "# Part I: \n",
    "\n",
    "## Intro to files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60e739",
   "metadata": {},
   "source": [
    "## 1.1 - We currently have five distinct data groups: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362b4ec",
   "metadata": {},
   "source": [
    "### 1.1.1 - A. (Sales notifications of all industrialized drugs that have restricted sales in Brazil, like antidepressants, antibiotics, etc.)\n",
    "\n",
    "Example: \"project_data\\raw_anvisa_files\\industrialized_meds\\EDA_Industrializados_201401.csv\"   \n",
    "(93 files in this dir ranging from 2014JAN ~ 2021NOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e86a53",
   "metadata": {},
   "source": [
    "### 1.1.2 - B. (Sales notifications of all compounded drugs that have restricted sales in Brazil, like antidepressants, antibiotics, etc.) \n",
    "\n",
    "Example: \"project_data\\raw_anvisa_files\\manipulated_meds\\EDA_Manipulados_201401.csv\"   \n",
    "(93 files in this dir ranging from 2014JAN ~ 2021NOV)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b7705",
   "metadata": {},
   "source": [
    "### 1.1.3 - C. (Notifications of all obituaries in Brazil)\n",
    "\n",
    "Example: \"project_data\\raw_sim_files\\Mortalidade_Geral_2014.csv\"  \n",
    "(8 files ranging yearly from 2014 to 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a873268",
   "metadata": {},
   "source": [
    "### 1.1.4 - D. (Brazilian cities' total area)\n",
    "Example:\"\\project_data\\ibge_demographic\\br_city_area.ods\"  \n",
    "(A single file from 2023) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100b4d7",
   "metadata": {},
   "source": [
    "### 1.1.5 - E. (Brazilian cities' population)\n",
    "Example: \"\\project_data\\ibge_demographic\\br_city_population.ods\"   \n",
    "(A single file from 2021) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0ed67",
   "metadata": {},
   "source": [
    "# Part II:\n",
    "\n",
    "# Checking column names in all paths:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded15d6",
   "metadata": {},
   "source": [
    "## 2.1 Defining all \"folder\" paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2151aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\raw_anvisa_files\\industrialized_meds\",\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\raw_anvisa_files\\manipulated_meds\",\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\raw_sim_files\",\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\ibge_demographic\\area\",\n",
    "    r\"(...)\\Drug_Resistant_Bacteria\\project_data\\ibge_demographic\\population\"   \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28669a35",
   "metadata": {},
   "source": [
    "## 2.2 Defining a function that iterates through files inside \"folder_paths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ff8a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(folder_path, extension=None):\n",
    "    if extension:\n",
    "        return [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(extension)]\n",
    "    else:\n",
    "        return [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de1feb",
   "metadata": {},
   "source": [
    "## 2.3 Defining a function to verify the integrity of column names and return the file indices of columns that have different names.\n",
    "\n",
    "#### I have also included another feature, namely chunk_size, with the goal of optimizing resource usage by not opening entire files at once. (Using 1k chunks is more than enough to retrieve column names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56517c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns_similarity(folder_paths, chunk_size=1000):\n",
    "    for folder_path in folder_paths:\n",
    "        file_paths = get_file_paths(folder_path)\n",
    "\n",
    "        if not file_paths:\n",
    "            raise ValueError(f\"Folder {folder_path} is empty. No files to process.\")\n",
    "\n",
    "        # Extract only the folder name\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        # Dictionary to store column names for each file within the folder\n",
    "        folder_columns = {}\n",
    "\n",
    " # Process each file within the folder\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith(\".csv\"):\n",
    "            reader = pd.read_csv(file_path, delimiter=';', encoding='latin-1', quotechar='\"', chunksize=chunk_size)\n",
    "            df_chunk = next(reader)\n",
    "            folder_columns[file_path] = df_chunk.columns.tolist()\n",
    "        elif file_path.endswith(\".xls\"):\n",
    "            for df_chunk in read_excel_in_chunks(file_path, chunk_size=chunk_size):\n",
    "                folder_columns[file_path] = df_chunk.columns.tolist()\n",
    "                break  # Only process the first chunk for column names\n",
    "        elif file_path.endswith(\".xlsx\"):\n",
    "            # For .xlsx files, continue using openpyxl\n",
    "            reader = pd.read_excel(file_path, engine='openpyxl', chunksize=chunk_size)\n",
    "            df_chunk = next(reader)\n",
    "            folder_columns[file_path] = df_chunk.columns.tolist()\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {file_path}. Skipping.\")\n",
    "\n",
    "    # Check if all files within the folder have the same columns\n",
    "    if len(set(map(tuple, folder_columns.values()))) == 1:\n",
    "        print(f\"\\U0001F7E2 All files in folder \\\"{folder_name}\\\" have the same columns.\\n\")  # Green circle emoji\n",
    "        return {'files_with_same_columns': list(range(len(file_paths))), 'files_with_different_columns': []}\n",
    "    else:\n",
    "        print(f\"\\U0001F534 Files in folder \\\"{folder_name}\\\" have different columns.\\n\")  # Red circle emoji\n",
    "\n",
    "        # Group files based on columns\n",
    "        grouped_files = {}\n",
    "        for index, (path, columns) in enumerate(folder_columns.items()):\n",
    "            key = tuple(columns)\n",
    "            if key not in grouped_files:\n",
    "                grouped_files[key] = [index]\n",
    "            else:\n",
    "                grouped_files[key].append(index)\n",
    "\n",
    "        return {'files_with_same_columns': grouped_files.get(next(iter(grouped_files)), []),\n",
    "                'files_with_different_columns': [index for index in range(len(file_paths)) if index not in grouped_files.get(next(iter(grouped_files)), [])]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f65f38",
   "metadata": {},
   "source": [
    "## 2.4 Usage example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17e198",
   "metadata": {},
   "source": [
    "### - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f03feffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the index of the folder to be processed (e.g., folder_paths[0] for the first folder)\n",
    "selected_folder_path_a = folder_paths[0]\n",
    "\n",
    "# Get all files in the selected folder\n",
    "all_files_a = get_file_paths(selected_folder_path_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3c9cf3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ All files in folder \"industrialized_meds\" have the same columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93], 'files_with_different_columns': []}\n"
     ]
    }
   ],
   "source": [
    "# Print the columns of the files in the selected folder\n",
    "result_columns = check_columns_similarity([selected_folder_path_a])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe4dfd",
   "metadata": {},
   "source": [
    "### - B)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1451191",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folder_path_b = folder_paths[1]\n",
    "all_files_b = get_file_paths(selected_folder_path_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24dd789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ All files in folder \"manipulated_meds\" have the same columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93], 'files_with_different_columns': []}\n"
     ]
    }
   ],
   "source": [
    "result_columns = check_columns_similarity([selected_folder_path_b])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f452d",
   "metadata": {},
   "source": [
    "### - C)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41ca4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folder_path_c = folder_paths[2]\n",
    "all_files_c = get_file_paths(selected_folder_path_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f56adf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ Files in folder \"raw_sim_files\" have different columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0, 1, 2, 3], 'files_with_different_columns': [4, 5, 6, 7]}\n"
     ]
    }
   ],
   "source": [
    "result_columns = check_columns_similarity([selected_folder_path_c])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93e65e",
   "metadata": {},
   "source": [
    "## The files in the above folder don't have standardized columns.\n",
    "#### We can keep this in mind when looking for errors during the subsequent data insertion into MSSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a2794",
   "metadata": {},
   "source": [
    "### - D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6e88c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folder_path_d = folder_paths[3]\n",
    "all_files_d = get_file_paths(selected_folder_path_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ca3ee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ All files in folder \"area\" have the same columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0], 'files_with_different_columns': []}\n"
     ]
    }
   ],
   "source": [
    "result_columns = check_columns_similarity([selected_folder_path_d])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8820a48",
   "metadata": {},
   "source": [
    "### - E)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folder_path_e = folder_paths[4]\n",
    "all_files_e = get_file_paths(selected_folder_path_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46bd84bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ All files in folder \"population\" have the same columns.\n",
      "\n",
      "Columns of one file: {'files_with_same_columns': [0], 'files_with_different_columns': []}\n"
     ]
    }
   ],
   "source": [
    "result_columns = check_columns_similarity([selected_folder_path_e])\n",
    "print(\"Columns of one file:\", result_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e03b59",
   "metadata": {},
   "source": [
    "# Part III:\n",
    "## Joining D and E files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef349b",
   "metadata": {},
   "source": [
    "## 3.1 - Doing a inner join with D and E\n",
    "\n",
    "D represents the local directory containing data for the total area of cities, while E represents the file containing the total population of cities. Since both files are relatively small, it makes sense to perform this operation with Pandas, conducting an inner join based on the city code, and subsequently modifying it for compatibility with an SQL-supported extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "986586e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ID CD_UF     NM_UF NM_UF_SIGLA   CD_MUN                 NM_MUN  AR_MUN_2022  \\\n",
      "0  1    11  Rond√¥nia          RO  1100015  Alta Floresta D'Oeste     7067.127   \n",
      "1  2    11  Rond√¥nia          RO  1100023              Ariquemes     4426.571   \n",
      "2  3    11  Rond√¥nia          RO  1100031                 Cabixi     1314.352   \n",
      "3  4    11  Rond√¥nia          RO  1100049                 Cacoal     3793.000   \n",
      "4  5    11  Rond√¥nia          RO  1100056             Cerejeiras     2783.300   \n",
      "\n",
      "   POPULA√á√ÉO ESTIMADA   \n",
      "0              22516.0  \n",
      "1             111148.0  \n",
      "2               5067.0  \n",
      "3              86416.0  \n",
      "4              16088.0  \n"
     ]
    }
   ],
   "source": [
    "# 3.1.1 Reading the data\n",
    "df_d = read_ods(folder_path_d, 1)\n",
    "df_e = read_ods(folder_path_e, 1)\n",
    "\n",
    "# 3.1.2 Merge the DataFrames based on the common column 'NM_MUN' in df_d and 'NOME DO MUNIC√çPIO' in df_e\n",
    "merged_df = df_d.merge(df_e[['NOME DO MUNIC√çPIO', ' POPULA√á√ÉO ESTIMADA ']], left_on='NM_MUN', right_on='NOME DO MUNIC√çPIO', how='inner')\n",
    "\n",
    "# 3.1.3 Drop the redundant 'NOME DO MUNIC√çPIO' column\n",
    "merged_df.drop(columns=['NOME DO MUNIC√çPIO'], inplace=True)\n",
    "\n",
    "# 3.1.4 Rename the merged DataFrame to 'city_area_population'\n",
    "city_area_population = merged_df\n",
    "\n",
    "# 3.1.5: Save the DataFrame as a CSV file\n",
    "\n",
    "# 3.1.6 Save the 'city_area_population' DataFrame as a CSV file in the same directory\n",
    "\n",
    "output_file_path= r\"(...)\\My_2ndEDA_Drug_Resistent_Bacteria\\project_data\\ibge_demographic\\city_area_population.csv\"\n",
    "\n",
    "city_area_population.to_csv(output_file_path, index=False)\n",
    "\n",
    "# 3.1.7 Display the first few rows of the DataFrame\n",
    "\n",
    "# 3.1.8 Print the first few rows of the 'city_area_population' DataFrame\n",
    "print(city_area_population.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd9e21",
   "metadata": {},
   "source": [
    "# Part IV: \n",
    "## 4.1 - Getting column dtypes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270c089",
   "metadata": {},
   "source": [
    "### 4.1.1 - For \"a\" files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eceb5a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANO_VENDA                   int64\n",
      "MES_VENDA                   int64\n",
      "UF_VENDA                   object\n",
      "MUNICIPIO_VENDA            object\n",
      "PRINCIPIO_ATIVO            object\n",
      "DESCRICAO_APRESENTACAO     object\n",
      "QTD_VENDIDA                 int64\n",
      "UNIDADE_MEDIDA             object\n",
      "CONSELHO_PRESCRITOR        object\n",
      "UF_CONSELHO_PRESCRITOR     object\n",
      "TIPO_RECEITUARIO          float64\n",
      "CID10                      object\n",
      "SEXO                      float64\n",
      "IDADE                     float64\n",
      "UNIDADE_IDADE             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "if all_files_a:\n",
    "    # Read the first CSV file in the list into a DataFrame\n",
    "    df = pd.read_csv(all_files_a[0], delimiter=';', encoding='latin-1', quotechar='\"')\n",
    "\n",
    "    # Get the data types of the columns\n",
    "    column_types = df.dtypes\n",
    "\n",
    "    # Print the data types\n",
    "    print(column_types)\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf273b",
   "metadata": {},
   "source": [
    "### 4.1.2 - For \"b\" files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab456676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANO_VENDA                           int64\n",
      "MES_VENDA                           int64\n",
      "UF_VENDA                           object\n",
      "MUNICIPIO_VENDA                    object\n",
      "PRINCIPIO_ATIVO                    object\n",
      "QTD_ATIVO_POR_UNID_FARMACOTEC      object\n",
      "UNIDADE_MEDIDA_PRINCIPIO_ATIVO     object\n",
      "QTD_UNIDADE_FARMACOTECNICA         object\n",
      "TIPO_UNIDADE_FARMACOTECNICA        object\n",
      "CID10                              object\n",
      "SEXO                              float64\n",
      "IDADE                             float64\n",
      "UNIDADE_IDADE                     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "if all_files_b:\n",
    "    # Read the second CSV file in the list into a DataFrame\n",
    "    df = pd.read_csv(all_files_b[1], delimiter=';', encoding='latin-1', quotechar='\"')\n",
    "\n",
    "    # Get the data types of the columns\n",
    "    column_types = df.dtypes\n",
    "\n",
    "    # Print the data types\n",
    "    print(column_types)\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f90557",
   "metadata": {},
   "source": [
    "### 4.1.3 - For \"c\" files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b1eb0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTADOR        int64\n",
      "ORIGEM          int64\n",
      "TIPOBITO        int64\n",
      "DTOBITO         int64\n",
      "HORAOBITO     float64\n",
      "NATURAL       float64\n",
      "CODMUNNATU    float64\n",
      "DTNASC        float64\n",
      "IDADE           int64\n",
      "SEXO            int64\n",
      "RACACOR       float64\n",
      "ESTCIV        float64\n",
      "ESC           float64\n",
      "ESC2010       float64\n",
      "SERIESCFAL    float64\n",
      "OCUP          float64\n",
      "CODMUNRES       int64\n",
      "LOCOCOR         int64\n",
      "CODESTAB      float64\n",
      "ESTABDESCR    float64\n",
      "CODMUNOCOR      int64\n",
      "IDADEMAE      float64\n",
      "ESCMAE        float64\n",
      "ESCMAE2010    float64\n",
      "SERIESCMAE    float64\n",
      "OCUPMAE       float64\n",
      "QTDFILVIVO    float64\n",
      "QTDFILMORT    float64\n",
      "GRAVIDEZ      float64\n",
      "SEMAGESTAC    float64\n",
      "GESTACAO      float64\n",
      "PARTO         float64\n",
      "OBITOPARTO    float64\n",
      "PESO          float64\n",
      "TPMORTEOCO    float64\n",
      "OBITOGRAV     float64\n",
      "OBITOPUERP    float64\n",
      "ASSISTMED     float64\n",
      "EXAME         float64\n",
      "CIRURGIA      float64\n",
      "NECROPSIA     float64\n",
      "LINHAA         object\n",
      "LINHAB         object\n",
      "LINHAC         object\n",
      "LINHAD         object\n",
      "LINHAII        object\n",
      "CAUSABAS       object\n",
      "CB_PRE        float64\n",
      "COMUNSVOIM    float64\n",
      "DTATESTADO    float64\n",
      "CIRCOBITO     float64\n",
      "ACIDTRAB      float64\n",
      "FONTE         float64\n",
      "NUMEROLOTE    float64\n",
      "TPPOS          object\n",
      "DTINVESTIG    float64\n",
      "CAUSABAS_O     object\n",
      "DTCADASTRO    float64\n",
      "ATESTANTE     float64\n",
      "STCODIFICA     object\n",
      "CODIFICADO     object\n",
      "VERSAOSIST     object\n",
      "VERSAOSCB     float64\n",
      "FONTEINV      float64\n",
      "DTRECEBIM     float64\n",
      "ATESTADO       object\n",
      "DTRECORIGA      int64\n",
      "CAUSAMAT       object\n",
      "ESCMAEAGR1    float64\n",
      "ESCFALAGR1    float64\n",
      "STDOEPIDEM    float64\n",
      "STDONOVA        int64\n",
      "DIFDATA         int64\n",
      "NUDIASOBCO    float64\n",
      "NUDIASOBIN    float64\n",
      "DTCADINV      float64\n",
      "TPOBITOCOR    float64\n",
      "DTCONINV      float64\n",
      "FONTES         object\n",
      "TPRESGINFO    float64\n",
      "TPNIVELINV     object\n",
      "NUDIASINF     float64\n",
      "DTCADINF      float64\n",
      "MORTEPARTO    float64\n",
      "DTCONCASO     float64\n",
      "FONTESINF     float64\n",
      "ALTCAUSA      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Setting pandas option that will let us see all columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "if all_files_c:\n",
    "    # Read the third CSV file in the list into a DataFrame\n",
    "    df = pd.read_csv(all_files_c[2], delimiter=';', encoding='latin-1', quotechar='\"')\n",
    "\n",
    "    # Get the data types of the columns\n",
    "    column_types = df.dtypes\n",
    "\n",
    "    # Print the data types\n",
    "    print(column_types)\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb39e9d",
   "metadata": {},
   "source": [
    "### 4.1.4 - For \"F\" files:\n",
    "#### (We refer to the output obtained by combining \"E\" and \"D\" as \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "038e96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the \"file_path_f\": \n",
    "\n",
    "file_path_f = r\"(...)\\Drug_Resistant_Bacteria\\project_data\\ibge_demographic\\city_area_population.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ffbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,CD_UF,NM_UF,NM_UF_SIGLA,CD_MUN,NM_MUN,AR_MUN_2022, POPULA√á√ÉO ESTIMADA     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# L√™ o arquivo CSV em um DataFrame\n",
    "df = pd.read_csv(file_path_f, delimiter=';', encoding='utf-8', quotechar='\"')\n",
    "\n",
    "# Obt√©m os tipos de dados das colunas\n",
    "column_types = df.dtypes\n",
    "\n",
    "# Imprime os tipos de dados\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a90491",
   "metadata": {},
   "source": [
    "# Part V: \"Dropping unnecessary columns\"\n",
    "#### (Or selecting desired columns, you choose, lol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b367c",
   "metadata": {},
   "source": [
    "## 5.1 - Defining desired columns for each file types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf84f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns_a = ['ANO_VENDA', 'MES_VENDA', 'UF_VENDA', 'MUNICIPIO_VENDA', 'PRINCIPIO_ATIVO', 'QTD_VENDIDA', 'UNIDADE_MEDIDA',\n",
    " 'CID10', 'SEXO', 'IDADE', 'UNIDADE_IDADE']\n",
    "\n",
    "desired_columns_b = ['ANO_VENDA', 'MES_VENDA', 'UF_VENDA', 'MUNICIPIO_VENDA', 'PRINCIPIO_ATIVO', \n",
    "                      'CID10', 'SEXO', 'IDADE', 'UNIDADE_IDADE']\n",
    "\n",
    "desired_columns_c = ['CONTADOR', 'DTOBITO', 'NATURAL', 'CODMUNNATU', 'IDADE', 'SEXO', 'CODMUNRES', 'CODESTAB',\n",
    " 'CODMUNOCOR', 'LINHAA', 'LINHAB', 'LINHAC', 'LINHAD', 'LINHAII', 'CAUSABAS','CAUSABAS_O',\n",
    " 'DTCADASTRO', 'ATESTADO', 'CAUSAMAT','STDOEPIDEM']\n",
    "\n",
    "desired_columns_d = ['ID','CD_UF','NM_UF','NM_UF_SIGLA','CD_MUN','NM_MUN','AR_MUN_2022']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ac978",
   "metadata": {},
   "source": [
    "# Part VI: Connecting to SQL server and creating target tables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1044d",
   "metadata": {},
   "source": [
    "## 6.1 - Defining function that creates table inside mssql: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "448f6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(engine, table_name, column_definitions, if_exists='fail'):\n",
    "    metadata = MetaData()\n",
    "    table = Table(table_name, metadata, *[Column(name, col_type) for name, col_type in column_definitions.items()])\n",
    "\n",
    "    try:\n",
    "        metadata.create_all(engine)\n",
    "        print(f'Table \"{table_name}\" created successfully.')\n",
    "    except IntegrityError as e:\n",
    "        if if_exists == 'replace':\n",
    "            metadata.drop_all(engine)\n",
    "            metadata.create_all(engine)\n",
    "            print(f'Table \"{table_name}\" replaced successfully.')\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d3ec9",
   "metadata": {},
   "source": [
    "## 6.2 - Defining engine parameters to connect to sql instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "027b0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_server', 'your_database', 'your_user', and 'your_password' with the correct information\n",
    "engine = create_engine(r'mssql+pyodbc://\"your_server\"/\"your_database\"?driver=ODBC+Driver+17+for+SQL+\"your_password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2306c",
   "metadata": {},
   "source": [
    "## 6.3 - Defining which dtypes for creating each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "288724b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of table names\n",
    "table_names = ['industrialized_meds', 'manipulated_meds', 'death_obituaries', 'demographics']  # Add desired column names here\n",
    "\n",
    "industrialized_meds = {\n",
    "    'ANO_VENDA': Integer(),\n",
    "    'MES_VENDA': Integer(),\n",
    "    'UF_VENDA': String(),\n",
    "    'MUNICIPIO_VENDA': String(),\n",
    "    'PRINCIPIO_ATIVO': String(),\n",
    "    'DESCRICAO_APRESENTACAO': String(),\n",
    "    'QTD_VENDIDA': Integer(),\n",
    "    'UNIDADE_MEDIDA': String(),\n",
    "    'CONSELHO_PRESCRITOR': String(),\n",
    "    'UF_CONSELHO_PRESCRITOR': String(),\n",
    "    'TIPO_RECEITUARIO': Float(),\n",
    "    'CID10': String(),\n",
    "    'SEXO': Float(),\n",
    "    'IDADE': Float(),\n",
    "    'UNIDADE_IDADE': Float()\n",
    "}\n",
    "\n",
    "manipulated_meds = {\n",
    "    'ANO_VENDA': Integer(),\n",
    "    'MES_VENDA': Integer(),\n",
    "    'UF_VENDA': String(),\n",
    "    'MUNICIPIO_VENDA': String(),\n",
    "    'PRINCIPIO_ATIVO': String(),\n",
    "    'QTD_ATIVO_POR_UNID_FARMACOTEC': String(),\n",
    "    'UNIDADE_MEDIDA_PRINCIPIO_ATIVO': String(),\n",
    "    'QTD_UNIDADE_FARMACOTECNICA': String(),\n",
    "    'TIPO_UNIDADE_FARMACOTECNICA': String(),\n",
    "    'CID10': String(),\n",
    "    'SEXO': Float(),\n",
    "    'IDADE': Float(),\n",
    "    'UNIDADE_IDADE': Float()\n",
    "}\n",
    "\n",
    "death_obituaries = {\n",
    " 'CONTADOR': Integer(),\n",
    "    'DTOBITO': Integer(),\n",
    "    'NATURAL': Float(),\n",
    "    'CODMUNNATU': Float(),\n",
    "    'IDADE': Integer(),\n",
    "    'SEXO': Integer(),\n",
    "    'CODMUNRES':Integer(),\n",
    "    'LOCOCOR': Integer(),\n",
    "    'CODESTAB': Float(),\n",
    "    'CODMUNOCOR':Integer(),\n",
    "    'LINHAA': String(),\n",
    "    'LINHAB': String(),\n",
    "    'LINHAC': String(),\n",
    "    'LINHAD': String(),\n",
    "    'LINHAII': String(),\n",
    "    'CAUSABAS': String(),\n",
    "    'CAUSABAS_O': String(),\n",
    "    'DTCADASTRO': Float(),\n",
    "    'ATESTADO': String(),\n",
    "    'CAUSAMAT': String(),\n",
    "    'STDOEPIDEM': Float()\n",
    "}\n",
    "\n",
    "demographics = {\n",
    "    'ID': Integer(),\n",
    "    'CD_UF': Integer(),\n",
    "    'NM_UF': String(),\n",
    "    'NM_UF_SIGLA': String(),\n",
    "    'CD_MUN': Integer(),\n",
    "    'NM_MUN': String(),\n",
    "    'AR_MUN_2022': Float(),\n",
    "    'POPULA√á√ÉO ESTIMADA': String()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8b3e9",
   "metadata": {},
   "source": [
    "# 5.3 - Calling function to create tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c670d013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table \"industrialized_meds\" created successfully.\n",
      "Table \"manipulated_meds\" created successfully.\n",
      "Table \"death_obituaries\" created successfully.\n",
      "Table \"demographics\" created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over table names and create tables\n",
    "for table_name, column_definitions in zip(table_names, [industrialized_meds, manipulated_meds, death_obituaries, demographics]):\n",
    "    create_table(engine, table_name, column_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b6415",
   "metadata": {},
   "source": [
    "# Part VI: \n",
    "## 6.0 - Counting number of rows in each file to verify data integrity after insertions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3e408",
   "metadata": {},
   "source": [
    "## 6.1 - Defining the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84ed68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_in_file(file_path, encoding='latin-1'):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter=';', encoding=encoding, quoting=csv.QUOTE_ALL)\n",
    "        row_count = len(df)\n",
    "        return row_count\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError: Could not read the file {file_path}.\")\n",
    "        return 0\n",
    "\n",
    "def create_report(folder_path, output_file_path, encoding='latin-1'):\n",
    "    row_counts = {}\n",
    "\n",
    "    # Get all CSV files in the folder\n",
    "    csv_files = get_file_paths(folder_path, extension='.csv')\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        row_count = count_rows_in_file(file_path, encoding)\n",
    "\n",
    "        # Extract year and month from the filename\n",
    "        match = re.search(r'_(\\d{4})(\\d{2})?\\.csv', os.path.basename(file_path))  # Updated regex pattern\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            month = match.group(2) or ''\n",
    "            key = f\"{year};{month}\"\n",
    "\n",
    "            row_counts[key] = row_count\n",
    "\n",
    "    # Convert row_counts to a DataFrame\n",
    "    report_df = pd.DataFrame(list(row_counts.items()), columns=[\"year;month\", \"count\"])\n",
    "\n",
    "    # Split the \"year;month\" column into two columns: \"year\" and \"month\"\n",
    "    report_df[[\"year\", \"month\"]] = report_df[\"year;month\"].str.split(\";\", expand=True)\n",
    "\n",
    "    # Now you can rename the columns\n",
    "    report_df = report_df[[\"year\", \"month\", \"count\"]]\n",
    "\n",
    "    # Save the report to the CSV file\n",
    "    report_df.to_csv(output_file_path, index=False, sep=';', decimal=',', header=True)\n",
    "\n",
    "    # Display the first few rows of the report\n",
    "    print(report_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ecf7a",
   "metadata": {},
   "source": [
    "## 6.2 Calling the report function:\n",
    "### We need to provide two arguments to generate the report that counts number of rows per file in each different dir: \n",
    "\n",
    "\"folder_path\" and \"output_file_path\": Example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025024b",
   "metadata": {},
   "source": [
    "## 'A' folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01abd1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month    count\n",
      "0  2014    01  4663124\n",
      "1  2014    02  4461102\n",
      "2  2014    03  4770799\n",
      "3  2014    04  4959036\n",
      "4  2014    05  5257119\n"
     ]
    }
   ],
   "source": [
    "output_report_path_a = r\"(...)\\Drug_Resistant_Bacteria\\project_data\\validation_reports\\report_a.csv\"\n",
    "create_report(selected_folder_path_a, output_report_path_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bf5c7",
   "metadata": {},
   "source": [
    "## 'B' folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7cc71ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month   count\n",
      "0  2014    01  272874\n",
      "1  2014    02  277087\n",
      "2  2014    03  272527\n",
      "3  2014    04  283229\n",
      "4  2014    05  295848\n"
     ]
    }
   ],
   "source": [
    "output_report_path_a = r\"(...)\\Drug_Resistant_Bacteria\\project_data\\validation_reports\\report_b.csv\"\n",
    "create_report(selected_folder_path_b, output_report_path_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629afbcb",
   "metadata": {},
   "source": [
    "## 'C' folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48914888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month    count\n",
      "0  2014        1227040\n",
      "1  2015        1264176\n",
      "2  2016        1309775\n",
      "3  2017        1312664\n",
      "4  2018        1316720\n"
     ]
    }
   ],
   "source": [
    "output_report_path_c = r\"(...)\\Drug_Resistant_Bacteria\\project_data\\validation_reports\\report_c.csv\"\n",
    "create_report(selected_folder_path_c, output_report_path_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b32bd2",
   "metadata": {},
   "source": [
    "## 'F' folder: \n",
    "\"The file in 'f' dir does not contain either years or months, so we only need to count the number of rows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d0caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in the file: 6225\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(file_path_f)\n",
    "    line_count = len(df)\n",
    "    print(f\"Total lines in the file: {line_count}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The file is empty or does not contain data.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error parsing the CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a356e",
   "metadata": {},
   "source": [
    "# Part VII: Inserting files into respectives tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c02bc",
   "metadata": {},
   "source": [
    "## 6.1 Defining function that inserts files into mssql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c74b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_csv_files_into_mssql(folder_path, table_name, desired_columns, engine, chunk_size=5000):\n",
    "    column_index_mapping = {col_name: i for i, col_name in enumerate(desired_columns)}\n",
    "\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    file_paths = get_file_paths(folder_path, extension=\".csv\")\n",
    "\n",
    "    # Abrir a conex√£o uma vez para todas as inser√ß√µes\n",
    "    with engine.connect() as conn:\n",
    "        for file_path in file_paths:\n",
    "            file_name = os.path.basename(file_path)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            chunks = pd.read_csv(file_path, delimiter=';', encoding='latin-1', quotechar='\"', chunksize=chunk_size)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                df_selected_columns = chunk[[col for col in chunk.columns if col in column_index_mapping]]\n",
    "\n",
    "                # Usar a conex√£o para inser√ß√£o\n",
    "                df_selected_columns.to_sql(table_name, con=conn, if_exists='append', index=False, method='multi')\n",
    "\n",
    "                print(f'Chunk {i+1} inserted for {file_name}.')\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            print(f'Time elapsed for {file_name}: {elapsed_time} seconds')\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    elapsed_time_total = end_time_total - start_time_total\n",
    "\n",
    "    print(f'Total time elapsed: {elapsed_time_total} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b152c",
   "metadata": {},
   "source": [
    "## Inserting files into 'industrialized_meds' table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2505459",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_a = insert_csv_files_into_mssql_(selected_folder_path_a, 'industrialized_meds', desired_columns_a.keys(), engine, chunk_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c88e9c",
   "metadata": {},
   "source": [
    "## Inserting files into 'manipulated_meds' table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6907ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_b = insert_csv_files_into_mssql(selected_folder_path_b, 'manipulated_meds', desired_columns_b.keys(), engine, chunk_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a34b1",
   "metadata": {},
   "source": [
    "## Inserting files into 'death_obituaries' table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19472581",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_c = insert_csv_files_into_mssql(selected_folder_path_c, 'death_obituaries', death_obituaries.keys(), engine, chunk_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a1bce",
   "metadata": {},
   "source": [
    "## Inserting files into 'demographics' table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429dd9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_f = insert_csv_files_into_mssql(folder_path_f, 'demographics', desired_columns_f, engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
